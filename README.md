# Emotion_Detector_and_Image_Analysis

## Overview
This project is an Emotion Detection and Image Analysis system that utilizes computer vision techniques to analyze human emotions based on body language and facial expressions. The model leverages **MediaPipe**, **OpenCV**, and **scikit-learn** to process images and detect emotional cues.

## Features
- ğŸ­ Real-time Emotion Detection using **MediaPipe**
- ğŸ“¸ Facial Expression and Body Language Analysis
- ğŸ¤– Uses Machine Learning for classification
- ğŸ“‚ Works with image and video input

## Installation
To run this project, install the required dependencies using:
```sh
pip install mediapipe opencv-python pandas scikit-learn
```

## Usage
### Running the Emotion Detector
1. Clone the repository and navigate to the project folder.
   ```sh
   git clone https://github.com/your-username/emotion-detector.git
   cd emotion-detector
   ```
2. Run the Jupyter Notebook using:
   ```sh
   jupyter notebook
   ```
3. Open the provided `.ipynb` file and execute the cells step by step.
4. Upload an image or video for analysis.
5. View the detected emotions in real time.

## Project Structure
```
Emotion-Detector/
â”‚â”€â”€ dataset/              # Sample images/videos for testing
â”‚â”€â”€ models/               # Pre-trained models (if any)
â”‚â”€â”€ Body Language Decoder Tutorial.ipynb  # Main notebook
â”‚â”€â”€ requirements.txt      # Dependencies
â”‚â”€â”€ README.md             # Project documentation
```

## Dependencies
- ğŸ Python 3.x
- ğŸ¥ MediaPipe
- ğŸ–¼ï¸ OpenCV
- ğŸ“Š Pandas
- ğŸ‹ï¸â€â™‚ï¸ Scikit-learn

## Future Enhancements
- ğŸš€ Integrating Deep Learning for better accuracy
- ğŸ“ˆ Expanding dataset for more emotion categories
- ğŸŒ Developing a real-time streaming web app

## Contributors
Developed by **Om Nanaware**

## License
ğŸ“œ This project is open-source and available under the MIT License.

